{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c9efab-d552-4548-9134-4a76f6bf56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df, args, add_year=False):\n",
    "    global training_features\n",
    "    features = copy.deepcopy(training_features)\n",
    "    df['dimuon_pt_log'] = np.log(df['dimuon_pt'])\n",
    "    df['jj_mass_log'] = np.log(df['jj_mass'])\n",
    "    if add_year and ('year' not in features):\n",
    "        features+=['year']\n",
    "    if not add_year and ('year' in features):\n",
    "        features = [t for t in features if t!='year']\n",
    "    for trf in features:\n",
    "        if trf not in df.columns:\n",
    "            print(f'Variable {trf} not found in training dataframe!')\n",
    "    return df, features\n",
    "    \n",
    "def dnn_training(df, args, model):\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Dense, Activation, Input, Dropout, Concatenate, Lambda, BatchNormalization\n",
    "    from tensorflow.keras import backend as K\n",
    "\n",
    "    def scale_data(inputs, model, label):\n",
    "        x_mean = np.mean(x_train[inputs].values,axis=0)\n",
    "        x_std = np.std(x_train[inputs].values,axis=0)\n",
    "        training_data = (x_train[inputs]-x_mean)/x_std\n",
    "        validation_data = (x_val[inputs]-x_mean)/x_std\n",
    "        np.save(f\"output/trained_models/{model}/scalers_{label}\", [x_mean, x_std])\n",
    "        return training_data, validation_data\n",
    "\n",
    "    nfolds = 4\n",
    "    classes = df.cls.unique()\n",
    "    cls_idx_map = {cls:idx for idx,cls in enumerate(classes)}\n",
    "    add_year = (args['year']=='')\n",
    "    df, features = prepare_features(df, args, add_year)\n",
    "    df['cls_idx'] = df['cls'].map(cls_idx_map)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"output/trained_models/{model}/\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"Training features: \", features)\n",
    "    for i in range(nfolds):\n",
    "        if args['year']=='':\n",
    "            label = f\"allyears_{args['label']}_{i}\"\n",
    "        else:\n",
    "            label = f\"allyears_{args['year']}_{args['label']}_{i}\"\n",
    "        \n",
    "        train_folds = [(i+f)%nfolds for f in [0,1]]\n",
    "        val_folds = [(i+f)%nfolds for f in [2]]\n",
    "        eval_folds = [(i+f)%nfolds for f in [3]]\n",
    "\n",
    "        print(f\"Train classifier #{i+1} out of {nfolds}\")\n",
    "        print(f\"Training folds: {train_folds}\")\n",
    "        print(f\"Validation folds: {val_folds}\")\n",
    "        print(f\"Evaluation folds: {eval_folds}\")\n",
    "        print(f\"Samples used: \",df.s.unique())\n",
    "\n",
    "        train_filter = df.event.mod(nfolds).isin(train_folds)\n",
    "        val_filter = df.event.mod(nfolds).isin(val_folds)\n",
    "        eval_filter = df.event.mod(nfolds).isin(eval_folds)\n",
    "        \n",
    "        other_columns = ['event', 'wgt_nominal']\n",
    "        \n",
    "        df_train = df[train_filter]\n",
    "        df_val = df[val_filter]\n",
    "        \n",
    "        x_train = df_train[features]\n",
    "        y_train = df_train['cls_idx']\n",
    "        x_val = df_val[features]\n",
    "        y_val = df_val['cls_idx']\n",
    "\n",
    "        df_train['cls_avg_wgt'] = 1.0\n",
    "        df_val['cls_avg_wgt'] = 1.0\n",
    "        for icls, cls in enumerate(classes):\n",
    "            train_evts = len(y_train[y_train==icls])\n",
    "            df_train.loc[y_train==icls,'cls_avg_wgt'] = df_train.loc[y_train==icls,'wgt_nominal'].values.mean()\n",
    "            df_val.loc[y_val==icls,'cls_avg_wgt'] = df_val.loc[y_val==icls,'wgt_nominal'].values.mean()\n",
    "            print(f\"{train_evts} training events in class {cls}\")\n",
    "\n",
    "        for smp in df_train.s.unique():\n",
    "            df_train.loc[df_train.s==smp,'smp_avg_wgt'] = df_train.loc[df_train.s==smp,'wgt_nominal'].values.mean()\n",
    "            df_val.loc[df_val.s==smp,'smp_avg_wgt'] = df_val.loc[df_val.s==smp,'wgt_nominal'].values.mean()\n",
    "            print(f\"{train_evts} training events in class {cls}\")        \n",
    "            \n",
    "#        df_train['training_wgt'] = df_train['wgt_nominal']/df_train['cls_avg_wgt']\n",
    "#        df_val['training_wgt'] = df_val['wgt_nominal']/df_val['cls_avg_wgt']\n",
    "        df_train['training_wgt'] = df_train['wgt_nominal']/df_train['smp_avg_wgt']\n",
    "        df_val['training_wgt'] = df_val['wgt_nominal']/df_val['smp_avg_wgt']\n",
    "\n",
    "        \n",
    "        # scale data\n",
    "        x_train, x_val = scale_data(features, model, label)\n",
    "        x_train[other_columns] = df_train[other_columns]\n",
    "        x_val[other_columns] = df_val[other_columns]\n",
    "\n",
    "        # load model\n",
    "        input_dim = len(features)\n",
    "        inputs = Input(shape=(input_dim,), name = label+'_input')\n",
    "        x = Dense(128, name = label+'_layer_1', activation='tanh')(inputs)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(64, name = label+'_layer_2', activation='tanh')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(32, name = label+'_layer_3', activation='tanh')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "#        x = Dense(8, name = label+'_layer_4', activation='tanh')(x)\n",
    "#        x = Dropout(0.2)(x)\n",
    "#        x = BatchNormalization()(x)\n",
    "#        x = Dense(8, name = label+'_layer_5', activation='tanh')(x)\n",
    "#        x = Dropout(0.2)(x)\n",
    "#        x = BatchNormalization()(x)def prepare_features(df, args, add_year=False):\n",
    "    global training_features\n",
    "    features = copy.deepcopy(training_features)\n",
    "    df['dimuon_pt_log'] = np.log(df['dimuon_pt'])\n",
    "    df['jj_mass_log'] = np.log(df['jj_mass'])\n",
    "    if add_year and ('year' not in features):\n",
    "        features+=['year']\n",
    "    if not add_year and ('year' in features):\n",
    "        features = [t for t in features if t!='year']\n",
    "    for trf in features:\n",
    "        if trf not in df.columns:\n",
    "            print(f'Variable {trf} not found in training dataframe!')\n",
    "    return df, features\n",
    "    \n",
    "      \n",
    "\n",
    "def evaluation(df, args):\n",
    "    if df.shape[0]==0: return df\n",
    "    for model in args['dnn_models']:\n",
    "        df = dnn_evaluation(df, model, args)\n",
    "    for model in args['bdt_models']:\n",
    "        df = bdt_evaluation(df, model, args)\n",
    "    return df\n",
    "\n",
    "        outputs = Dense(1, name = label+'_output',  activation='sigmoid')(x)\n",
    "\n",
    "        dnn = Model(inputs=inputs, outputs=outputs)\n",
    "        dnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "        dnn.summary()\n",
    "\n",
    "#        history = dnn.fit(x_train[features], y_train, epochs=100, batch_size=1024,\\\n",
    "#                            sample_weight=df_train['training_wgt'].values, verbose=1,\\\n",
    "#                            validation_data=(x_val[features], y_val, df_val['training_wgt'].values), shuffle=True)\n",
    "        history = dnn.fit(x_train[features], y_train, epochs=100, batch_size=1024, verbose=1,\\\n",
    "                            validation_data=(x_val[features], y_val), shuffle=True)\n",
    "\n",
    "        util.save(history.history, f\"output/trained_models/{model}/history_{label}.coffea\")\n",
    "        dnn.save(f\"output/trained_models/{model}/dnn_{label}.h5\")        \n",
    "\n",
    "def evaluation(df, args):\n",
    "    if df.shape[0]==0: return df\n",
    "    for model in args['dnn_models']:\n",
    "        df = dnn_evaluation(df, model, args)\n",
    "    for model in args['bdt_models']:\n",
    "        df = bdt_evaluation(df, model, args)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5790f4c8-66ba-4599-bf37-fd0c5b1e64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_feat, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.output = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        output = F.sigmoid(x)\n",
    "        return output\n",
    "\n",
    "def scale_data(inputs, model, label):\n",
    "    x_mean = np.mean(x_train[inputs].values,axis=0)\n",
    "    x_std = np.std(x_train[inputs].values,axis=0)\n",
    "    training_data = (x_train[inputs]-x_mean)/x_std\n",
    "    validation_data = (x_val[inputs]-x_mean)/x_std\n",
    "    np.save(f\"output/trained_models/{model}/scalers_{label}\", [x_mean, x_std])\n",
    "    return training_data, validation_data\n",
    "\n",
    "training_features = [\n",
    "    'dimuon_mass', 'dimuon_pt', 'dimuon_pt_log', 'dimuon_eta', 'dimuon_mass_res', 'dimuon_mass_res_rel',\\\n",
    "     'dimuon_cos_theta_cs', 'dimuon_phi_cs',\n",
    "     'jet1_pt', 'jet1_eta', 'jet1_phi', 'jet1_qgl', 'jet2_pt', 'jet2_eta', 'jet2_phi', 'jet2_qgl',\\\n",
    "     'jj_mass', 'jj_mass_log', 'jj_dEta', 'rpt', 'll_zstar_log', 'mmj_min_dEta', 'nsoftjets5', 'htsoft2'\n",
    "                    ]\n",
    "# TODO: input year as features and other stuff\n",
    "def dnn_training(df, args, model):\n",
    "    nfolds = 4\n",
    "    classes = df.cls.unique()\n",
    "    cls_idx_map = {cls:idx for idx,cls in enumerate(classes)}\n",
    "    add_year = (args['year']=='')\n",
    "    df, features = prepare_features(df, args, add_year)\n",
    "    df['cls_idx'] = df['cls'].map(cls_idx_map)\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"output/trained_models/{model}/\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"Training features: \", features)\n",
    "    for i in range(nfolds):\n",
    "        if args['year']=='':\n",
    "            label = f\"allyears_{args['label']}_{i}\"\n",
    "        else:\n",
    "            label = f\"allyears_{args['year']}_{args['label']}_{i}\"\n",
    "        \n",
    "        train_folds = [(i+f)%nfolds for f in [0,1]]\n",
    "        val_folds = [(i+f)%nfolds for f in [2]]\n",
    "        eval_folds = [(i+f)%nfolds for f in [3]]\n",
    "\n",
    "        print(f\"Train classifier #{i+1} out of {nfolds}\")\n",
    "        print(f\"Training folds: {train_folds}\")\n",
    "        print(f\"Validation folds: {val_folds}\")\n",
    "        print(f\"Evaluation folds: {eval_folds}\")\n",
    "        print(f\"Samples used: \",df.s.unique())\n",
    "\n",
    "        train_filter = df.event.mod(nfolds).isin(train_folds)\n",
    "        val_filter = df.event.mod(nfolds).isin(val_folds)\n",
    "        eval_filter = df.event.mod(nfolds).isin(eval_folds)\n",
    "        \n",
    "        other_columns = ['event', 'wgt_nominal']\n",
    "        \n",
    "        df_train = df[train_filter]\n",
    "        df_val = df[val_filter]\n",
    "        \n",
    "        x_train = df_train[features]\n",
    "        y_train = df_train['cls_idx']\n",
    "        x_val = df_val[features]\n",
    "        y_val = df_val['cls_idx']\n",
    "\n",
    "        df_train['cls_avg_wgt'] = 1.0\n",
    "        df_val['cls_avg_wgt'] = 1.0\n",
    "        for icls, cls in enumerate(classes):\n",
    "            train_evts = len(y_train[y_train==icls])\n",
    "            df_train.loc[y_train==icls,'cls_avg_wgt'] = df_train.loc[y_train==icls,'wgt_nominal'].values.mean()\n",
    "            df_val.loc[y_val==icls,'cls_avg_wgt'] = df_val.loc[y_val==icls,'wgt_nominal'].values.mean()\n",
    "            print(f\"{train_evts} training events in class {cls}\")\n",
    "\n",
    "        for smp in df_train.s.unique():\n",
    "            df_train.loc[df_train.s==smp,'smp_avg_wgt'] = df_train.loc[df_train.s==smp,'wgt_nominal'].values.mean()\n",
    "            df_val.loc[df_val.s==smp,'smp_avg_wgt'] = df_val.loc[df_val.s==smp,'wgt_nominal'].values.mean()\n",
    "            print(f\"{train_evts} training events in class {cls}\")        \n",
    "            \n",
    "#        df_train['training_wgt'] = df_train['wgt_nominal']/df_train['cls_avg_wgt']\n",
    "#        df_val['training_wgt'] = df_val['wgt_nominal']/df_val['cls_avg_wgt']\n",
    "        df_train['training_wgt'] = df_train['wgt_nominal']/df_train['smp_avg_wgt']\n",
    "        df_val['training_wgt'] = df_val['wgt_nominal']/df_val['smp_avg_wgt']\n",
    "\n",
    "        \n",
    "        # scale data\n",
    "        x_train, x_val = scale_data(features, model, label)\n",
    "        x_train[other_columns] = df_train[other_columns]\n",
    "        x_val[other_columns] = df_val[other_columns]\n",
    "\n",
    "        # load model\n",
    "        input_dim = len(features)\n",
    "        inputs = Input(shape=(input_dim,), name = label+'_input')\n",
    "        x = Dense(128, name = label+'_layer_1', activation='tanh')(inputs)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(64, name = label+'_layer_2', activation='tanh')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(32, name = label+'_layer_3', activation='tanh')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "#        x = Dense(8, name = label+'_layer_4', activation='tanh')(x)\n",
    "#        x = Dropout(0.2)(x)\n",
    "#        x = BatchNormalization()(x)\n",
    "#        x = Dense(8, name = label+'_layer_5', activation='tanh')(x)\n",
    "#        x = Dropout(0.2)(x)\n",
    "#        x = BatchNormalization()(x)\n",
    "        outputs = Dense(1, name = label+'_output',  activation='sigmoid')(x)\n",
    "\n",
    "        dnn = Model(inputs=inputs, outputs=outputs)\n",
    "        dnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "        dnn.summary()\n",
    "\n",
    "#        history = dnn.fit(x_train[features], y_train, epochs=100, batch_size=1024,\\\n",
    "#                            sample_weight=df_train['training_wgt'].values, verbose=1,\\\n",
    "#                            validation_data=(x_val[features], y_val, df_val['training_wgt'].values), shuffle=True)\n",
    "        history = dnn.fit(x_train[features], y_train, epochs=100, batch_size=1024, verbose=1,\\\n",
    "                            validation_data=(x_val[features], y_val), shuffle=True)\n",
    "\n",
    "        util.save(history.history, f\"output/trained_models/{model}/history_{label}.coffea\")\n",
    "        dnn.save(f\"output/trained_models/{model}/dnn_{label}.h5\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9546fae6-a4c1-443b-8789-34a655022039",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events dimuon_mass: [126, 125, 122, 124, 125, 128, 121, 129, ..., 125, 128, 125, 126, 120, 125, 125]\n",
      "features: ['dimuon_mass', 'dimuon_pt', 'dimuon_pt_log', 'dimuon_eta', 'dimuon_cos_theta_cs', 'dimuon_phi_cs', 'jet1_pt_nominal', 'jet1_eta_nominal', 'jet1_phi_nominal', 'jet1_qgl_nominal', 'jet2_pt_nominal', 'jet2_eta_nominal', 'jet2_phi_nominal', 'jet2_qgl_nominal', 'jj_mass_nominal', 'jj_mass_log_nominal', 'jj_dEta_nominal', 'rpt_nominal', 'll_zstar_log_nominal', 'mmj_min_dEta_nominal', 'nsoftjets5_nominal', 'htsoft2_nominal']\n",
      "df:        dimuon_mass   dimuon_pt  dimuon_pt_log  dimuon_eta  \\\n",
      "entry                                                       \n",
      "0       126.306807   64.834205       4.171833    0.265019   \n",
      "1       124.914562  181.915749       5.203544    1.286440   \n",
      "2       121.604270   65.237430       4.178033   -1.824468   \n",
      "3       124.125347   51.643715       3.944368   -1.838996   \n",
      "4       125.454525  192.410311       5.259630    0.339478   \n",
      "\n",
      "       dimuon_cos_theta_cs  dimuon_phi_cs  jet1_pt_nominal  jet1_eta_nominal  \\\n",
      "entry                                                                          \n",
      "0                 0.819875      -1.353765         85.43750          2.584961   \n",
      "1                 0.220241       0.937727        127.12500         -2.458496   \n",
      "2                -0.168413       1.447098         75.68750         -3.244141   \n",
      "3                -0.674657       2.017473         55.65625          1.350098   \n",
      "4                 0.728057      -0.412657        153.37500         -1.985596   \n",
      "\n",
      "       jet1_phi_nominal  jet1_qgl_nominal  ...  jet2_phi_nominal  \\\n",
      "entry                                      ...                     \n",
      "0             -2.403809          0.959961  ...          1.438232   \n",
      "1             -0.215363          0.910645  ...          0.615112   \n",
      "2              1.620117          0.805664  ...         -0.569824   \n",
      "3              1.517822          0.951172  ...         -2.600098   \n",
      "4              1.916504          0.889160  ...          1.129395   \n",
      "\n",
      "       jet2_qgl_nominal  jj_mass_nominal  jj_mass_log_nominal  \\\n",
      "entry                                                           \n",
      "0              0.944336       565.345825             6.337438   \n",
      "1              0.853027      1840.167847             7.517612   \n",
      "2              0.993652       794.184631             6.677316   \n",
      "3              0.087952       637.887390             6.458162   \n",
      "4              0.947266      1160.331909             7.056461   \n",
      "\n",
      "       jj_dEta_nominal  rpt_nominal  ll_zstar_log_nominal  \\\n",
      "entry                                                       \n",
      "0             4.229248     0.058065             -2.494844   \n",
      "1             5.594727     0.065257             -1.957104   \n",
      "2             5.147217     0.020255             -2.360560   \n",
      "3             4.878906     0.004978             -4.037189   \n",
      "4             4.764404     0.056224             -3.773129   \n",
      "\n",
      "       mmj_min_dEta_nominal  nsoftjets5_nominal  htsoft2_nominal  \n",
      "entry                                                             \n",
      "0                  1.909306                   1        20.652130  \n",
      "1                  1.849791                   1        41.185150  \n",
      "2                  1.419673                   2        39.361404  \n",
      "3                  1.689813                   2        26.083374  \n",
      "4                  2.325073                   0        -0.022186  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import dask_awkward as dak\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import glob\n",
    "\n",
    "base_path = f\"/depot/cms/users/yun79/hmm/copperheadV1clean/V2_Dec22_HEMVetoOnZptOn_RerecoBtagSF_XS_Rereco/stage1_output/2018/f1_0/\"\n",
    "\n",
    "# def getParquetFiles(path):\n",
    "    # return glob.glob(path)\n",
    "\n",
    "def fillEventNans(events):\n",
    "    for field in events.fields:\n",
    "        if \"phi\" in field:\n",
    "            events[field] = ak.fill_none(events[field], value=-10) # we're working on a DNN, so significant deviation may be warranted\n",
    "        else: # for all other fields (this may need to be changed)\n",
    "            events[field] = ak.fill_none(events[field], value=0)\n",
    "    return events\n",
    "\n",
    "# def replaceSidebandMass(events):\n",
    "#     for field in events.fields:\n",
    "#         if \"phi\" in field:\n",
    "#             events[field] = ak.fill_none(events[field], value=-1)\n",
    "#         else: # for all other fields (this may need to be changed)\n",
    "#             events[field] = ak.fill_none(events[field], value=0)\n",
    "#     return events\n",
    "\n",
    "def applyCatAndFeatFilter(events, features: list, region=\"h-peak\", category=\"vbf\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # apply category filter\n",
    "    dimuon_mass = events.dimuon_mass\n",
    "    if region ==\"h-peak\":\n",
    "        region = (dimuon_mass > 115.03) & (dimuon_mass < 135.03)\n",
    "    elif region ==\"h-sidebands\":\n",
    "        region = ((dimuon_mass > 110) & (dimuon_mass < 115.03)) | ((dimuon_mass > 135.03) & (dimuon_mass < 150))\n",
    "    elif region ==\"signal\":\n",
    "        region = (dimuon_mass >= 110) & (dimuon_mass <= 150.0)\n",
    "    \n",
    "    if category.lower() == \"vbf\":\n",
    "        btag_cut =ak.fill_none((events.nBtagLoose_nominal >= 2), value=False) | ak.fill_none((events.nBtagMedium_nominal >= 1), value=False)\n",
    "        cat_cut = (events.jj_mass_nominal > 400) & (events.jj_dEta_nominal > 2.5) & (events.jet1_pt_nominal > 35) \n",
    "        cat_cut = cat_cut & (~btag_cut) # btag cut is for VH and ttH categories\n",
    "    elif category.lower()== \"ggh\":\n",
    "        btag_cut =ak.fill_none((events.nBtagLoose_nominal >= 2), value=False) | ak.fill_none((events.nBtagMedium_nominal >= 1), value=False)\n",
    "        cat_cut = (events.jj_mass_nominal > 400) & (events.jj_dEta_nominal > 2.5)\n",
    "        cat_cut = cat_cut & (~btag_cut) # btag cut is for VH and ttH categories\n",
    "    else: # no category cut is applied\n",
    "        cat_cut = ak.ones_like(dimuon_mass, dtype=\"bool\")\n",
    "    cat_cut = ak.fill_none(cat_cut, value=False)\n",
    "    cat_filter = (\n",
    "        cat_cut & \n",
    "        region \n",
    "    )\n",
    "    events = events[cat_filter] # apply the category filter\n",
    "    print(f\"events dimuon_mass: {events.dimuon_mass.compute()}\")\n",
    "    # apply the feature filter (so the ak zip only contains features we are interested)\n",
    "    print(f\"features: {features}\")\n",
    "    events = ak.zip({field : events[field] for field in features}) \n",
    "    return events\n",
    "\n",
    "\n",
    "def prepare_features(events, features, variation=\"nominal\"):\n",
    "    features_var = []\n",
    "    for trf in features:\n",
    "        if \"soft\" in trf:\n",
    "            variation_current = \"nominal\"\n",
    "        else:\n",
    "            variation_current = variation\n",
    "        \n",
    "        if f\"{trf}_{variation_current}\" in events.fields:\n",
    "            features_var.append(f\"{trf}_{variation_current}\")\n",
    "        elif trf in events.fields:\n",
    "            features_var.append(trf)\n",
    "        else:\n",
    "            print(f\"Variable {trf} not found in training dataframe!\")\n",
    "    return features_var\n",
    "\n",
    "def preprocess(base_path, region=\"h-peak\", category=\"vbf\"):\n",
    "    # training_features = [\n",
    "    #     \"dimuon_mass\",\n",
    "    #     \"dimuon_pt\",\n",
    "    #     \"dimuon_pt_log\",\n",
    "    #     \"dimuon_eta\",\n",
    "    #     # \"dimuon_ebe_mass_res\",\n",
    "    #     # \"dimuon_ebe_mass_res_rel\",\n",
    "    #     # \"dimuon_cos_theta_cs\",\n",
    "    #     # \"dimuon_phi_cs\",\n",
    "    #     \"dimuon_pisa_mass_res\",\n",
    "    #     \"dimuon_pisa_mass_res_rel\",\n",
    "    #     \"dimuon_cos_theta_cs_pisa\",\n",
    "    #     \"dimuon_phi_cs_pisa\",\n",
    "    #     \"jet1_pt\",\n",
    "    #     \"jet1_eta\",\n",
    "    #     \"jet1_phi\",\n",
    "    #     \"jet1_qgl\",\n",
    "    #     \"jet2_pt\",\n",
    "    #     \"jet2_eta\",\n",
    "    #     \"jet2_phi\",\n",
    "    #     \"jet2_qgl\",\n",
    "    #     \"jj_mass\",\n",
    "    #     \"jj_mass_log\",\n",
    "    #     \"jj_dEta\",\n",
    "    #     \"rpt\",\n",
    "    #     \"ll_zstar_log\",\n",
    "    #     \"mmj_min_dEta\",\n",
    "    #     \"nsoftjets5\",\n",
    "        # \"htsoft2\",\n",
    "        # \"year\",\n",
    "    # ]\n",
    "    training_features = [\n",
    "        'dimuon_mass', 'dimuon_pt', 'dimuon_pt_log', 'dimuon_eta', \\\n",
    "         'dimuon_cos_theta_cs', 'dimuon_phi_cs',\n",
    "         'jet1_pt', 'jet1_eta', 'jet1_phi', 'jet1_qgl', 'jet2_pt', 'jet2_eta', 'jet2_phi', 'jet2_qgl',\\\n",
    "         'jj_mass', 'jj_mass_log', 'jj_dEta', 'rpt', 'll_zstar_log', 'mmj_min_dEta', 'nsoftjets5', 'htsoft2'\n",
    "    ]\n",
    "    # TODO: add mixup\n",
    "    # sig and bkg processes defined at line 1976 of AN-19-124. IDK why ggH is not included here\n",
    "    sig_processes = [\"vbf_powheg_dipole\"]\n",
    "    bkg_processes = [\"dy_M-100To200\", \"ewk_lljj_mll105_160_ptj0\",\"ttjets_dl\",\"ttjets_sl\"]\n",
    "    \n",
    "    filenames = []\n",
    "    for process in sig_processes:\n",
    "        filenames += glob.glob(f\"{base_path}/{process}/*/*.parquet\")\n",
    "    # print(filenames)\n",
    "    events = dak.from_parquet(filenames)\n",
    "    training_features = prepare_features(events, training_features) # add variation to features\n",
    "    events = applyCatAndFeatFilter(events, training_features, region=region, category=category)\n",
    "    events = fillEventNans(events)\n",
    "    df = ak.to_dataframe(events.compute())\n",
    "    print(f\"df: {df.head()}\")\n",
    "    # turn to pandas df add label (signal=1, bkg=0)\n",
    "    # merge sig and bkg dfs\n",
    "    # calculate the scale, save it\n",
    "    # save the resulting df for training\n",
    "    \n",
    "    \n",
    "    filenames = []\n",
    "    for process in bkg_processes:\n",
    "        filenames += glob.glob(f\"{base_path}/{process}/*/*.parquet\")\n",
    "    # print(filenames)\n",
    "    bkg_events = dak.from_parquet(filenames)\n",
    "\n",
    "    # fillnan values\n",
    "preprocess(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd45d09-72b5-486d-a880-291d97f3acce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filenames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfilenames\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filenames' is not defined"
     ]
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a55fa5-1b3a-494e-9e11-a218b7323d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea_latest",
   "language": "python",
   "name": "coffea_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
